{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install tensorflow 2.8\n",
    "!pip install tensorflow==2.8.0\n",
    "#vsc pakai tanda %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ambil link project dataset\n",
    "url = 'https://raw.githubusercontent.com/RashidMaulana/Anticede/main/Machine%20Learning/data_2/data%20-%20datatest_2.csv'\n",
    "dataset = pd.read_csv(url)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change text to lowercase\n",
    "def proses_data(text):\n",
    "    newText = text.lower()\n",
    "    return newText\n",
    "#ini case kalau semua mau ngerun semua data Komputerku kentang maka ngefreeze)\n",
    "# dataset['comment_text'].apply(lambda x: print(x))\n",
    "#ini case kalau semua mau ngerun 5 data pertama\n",
    "testData = dataset['original_text']\n",
    "testData = testData.apply(lambda x: proses_data(x))\n",
    "\n",
    "#menghilangkan link dan tag html\n",
    "import re\n",
    "def link_remover(text):\n",
    "    url_regex = re.compile(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n",
    "    return url_regex.sub('', text)\n",
    "testData = testData.apply(lambda x: link_remover(x))\n",
    "\n",
    "#menghilangkan kalimat bersama tag\n",
    "import re\n",
    "def tag_remover(text):\n",
    "    tag_regex = re.compile(r'<[^>]+>')\n",
    "    return tag_regex.sub('', text)\n",
    "testData = testData.apply(lambda x: tag_remover(x))\n",
    "\n",
    "# menghilangkan tanda baca dan karakter spesial\n",
    "import re\n",
    "def karakter_spesial(text):\n",
    "    newText = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
    "    return newText\n",
    "testData = testData.apply(lambda x: karakter_spesial(x))\n",
    "\n",
    "# menghilangkan spasi berlebihan\n",
    "def Spasi_berlebihan(text):  \n",
    "    newText = re.sub('\\s', ' ', text)\n",
    "    return newText\n",
    "testData = testData.apply(lambda x: Spasi_berlebihan(x))\n",
    "\n",
    "# menghilangkan angka berlebih\n",
    "def angka_berlebihan(text):\n",
    "    newText = re.sub('[0-9]', '', text)\n",
    "    return newText\n",
    "testData = testData.apply(lambda x: angka_berlebihan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['pornografi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sara'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['radikalisme'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['pencemaran_nama_baik'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oov tokenizer\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(testData)\n",
    "word_index = tokenizer.word_index #menampilkan tabel kata yang ada di dataset\n",
    "print(f'number of words in word_index: {len(word_index)}')\n",
    "# Print the word index\n",
    "print(f'word_index: {word_index}')\n",
    "print()\n",
    "# Generate and pad the sequences\n",
    "sequences = tokenizer.texts_to_sequences(testData)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "# Print a sample headline\n",
    "index = 2\n",
    "print(f'sample headline: {testData[index]}')\n",
    "print(f'padded sequence: {padded[index]}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menampilkannya dalam bentuk integer\n",
    "sequences = tokenizer.texts_to_sequences(testData)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melatih data train dengan tensorflow\n",
    "url_train_data = 'https://raw.githubusercontent.com/RashidMaulana/Anticede/main/Machine%20Learning/data_2/data%20-%20datatrain_2.csv'\n",
    "train_data = pd.read_csv(url_train_data)\n",
    "\n",
    "# Change text to lowercase\n",
    "def proses_data(text):\n",
    "    newText = text.lower()\n",
    "    return newText\n",
    "traindata = train_data['original_text']\n",
    "traindata = traindata.apply(lambda x: proses_data(x))\n",
    "\n",
    "#menghilangkan link dan tag html\n",
    "def link_remover(text):\n",
    "    url_regex = re.compile(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n",
    "    return url_regex.sub('', text)\n",
    "traindata = traindata.apply(lambda x: link_remover(x))\n",
    "\n",
    "#menghilangkan kalimat bersama @name\n",
    "def name_remover(text):\n",
    "    name_regex = re.compile(r'@[\\w_-]+')\n",
    "    return name_regex.sub(' ', text)\n",
    "traindata = traindata.apply(lambda x: name_remover(x))\n",
    "\n",
    "#menghilangkan tanda baca dan karakter spesial\n",
    "def karakter_spesial(text):\n",
    "    newText = re.sub('[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    return newText\n",
    "traindata = traindata.apply(lambda x: karakter_spesial(x))\n",
    "\n",
    "#menghilang angka yang nempel dengan kalimat\n",
    "def angka_berlebihan(text):\n",
    "    newText = re.sub('[0-9]', '', text)\n",
    "    return newText\n",
    "traindata = traindata.apply(lambda x: angka_berlebihan(x))\n",
    "\n",
    "#menghilangkan spasi berlebihan\n",
    "def Spasi_berlebihan(text):  \n",
    "    newText = re.sub('\\s+', '', text)\n",
    "    newText = re.sub('^\\s+', '', text)\n",
    "    return newText\n",
    "traindata = traindata.apply(lambda x: Spasi_berlebihan(x))\n",
    "traindata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#initialize the tokenizer class\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(traindata)\n",
    "word_index = tokenizer.word_index\n",
    "print(f'number of words in word_index: {len(word_index)}')\n",
    "print(f'word_index: {word_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#initialize the sequences list\n",
    "sequences = tokenizer.texts_to_sequences(traindata)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(f'shape of padded sequences: {padded.shape}')\n",
    "label = train_data['pornografi']\n",
    "label.head()\n",
    "#print the sample headline\n",
    "index = 2\n",
    "print(f'sample headline: {traindata[index]}')\n",
    "inputs = padded[:-1]\n",
    "labels = padded[1:]\n",
    "# Print the shape of inputs and labels\n",
    "print(f'shape of inputs: {inputs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "Embedding_Dim = 100\n",
    "lstm = 128\n",
    "dense = 64\n",
    "#build the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, input_length=padded.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "##sepertinya aku salah di model trainingnya coba kamu ubah2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
